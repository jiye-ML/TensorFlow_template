import tflearnfrom tflearn.layers.core import *from tflearn.layers.conv import *from tflearn.data_utils import *from tflearn.layers.normalization import *from tflearn.layers.estimator import regression# CIFAR-10 Datasetfrom tflearn.datasets import cifar10(X, Y), (X_test, Y_test) = cifar10.load_data(dirname='./data/cifar-10-batches-py')Y = to_categorical(Y, 10)Y_test = to_categorical(Y_test, 10)def data_manager_h5py():    # create a hdf5 dataset from CIFAT-10 numpy array    import h5py    h5f = h5py.File('data.h5', 'w')    h5f.create_dataset('cifar10_X', data=X)    h5f.create_dataset('cifar10_Y', data=Y)    h5f.create_dataset('cifar10_X_test', data=X_test)    h5f.create_dataset('cifar10_Y_test', data=Y_test)    h5f.close()    h5f = h5py.File('data.h5', 'r')    X = h5f['cifar10_X']    Y = h5f['cifar10_Y']    X_test = h5f['cifar10_X_test']    Y_test = h5f['cifar10_Y_test']    # build network    network = input_data(shape=[None, 32, 32, 3], dtype=tf.float32)    network = conv_2d(network, 32, 3, activation='relu')    network = max_pool_2d(network, 2)    network = conv_2d(network, 64, 3, activation='relu')    network = conv_2d(network, 64, 3, activation='relu')    network = max_pool_2d(network, 2)    network = fully_connected(network, 512, activation='relu')    network = dropout(network, 0.5)    network = fully_connected(network, 10, activation='softmax')    network = regression(network, optimizer='adam', loss='categorical_crossentropy', learning_rate=0.001)    # training    model = tflearn.DNN(network, tensorboard_verbose=0)    model.fit(X, Y, n_epoch=50, shuffle=True, validation_set=(X_test, Y_test),              show_metric=True, batch_size=96, run_id='cifar10_cnn')def data_manager_dask():    # create DASK array using numpy arrays    # (Note that can work HDF5 Dataset too)    import dask.array as da    X = da.from_array(np.asarray(X), chunks=(1000, 1000, 1000, 1000))    Y = da.from_array(np.asarray(Y), chunks=(1000, 1000, 1000, 1000))    X_test = da.from_array(np.asarray(X_test), chunks=(1000, 1000, 1000, 1000))    Y_test = da.from_array(np.asarray(Y_test), chunks=(1000, 1000, 1000, 1000))    # build network    network = input_data(shape=[None, 32, 32, 3])    network = conv_2d(network, 32, 3, activation='relu')    network = max_pool_2d(network, 2)    network = dropout(network, 0.75)    network = conv_2d(network, 64, 3, activation='relu')    network = conv_2d(network, 64, 3, activation='relu')    network = max_pool_2d(network, 2)    network = dropout(network, 0.5)    network = fully_connected(network, 512, activation='relu')    network = dropout(network, 0.5)    network = fully_connected(network, 10, activation='softmax')    network = regression(network, optimizer='adam', loss='categorical_crossentropy', learning_rate=0.001)    # training    model = tflearn.DNN(network, tensorboard_verbose=0)    model.fit(X, Y, n_epoch=50, shuffle=True, validation_set=(X_test, Y_test), show_metric=True,              batch_size=96, run_id='cifar10_cnn')class Data_preprocessing_augmentation:    '''     When training a model,      the defined pre-processing methods will be applied at both training and testing time.      Note that DataAugmentation is similar to DataPreprocessing,      but only applies at training time.    '''    def data_preprocessing_augmentation(self):        # Real-time image preprocessing        img_prep = tflearn.ImagePreprocessing()        # Zero Center (With mean computed over the whole dataset)        img_prep.add_featurewise_zero_center()        # STD Normalization (With std computed over the whole dataset)        img_prep.add_featurewise_stdnorm()        # Real-time data augmentation        img_aug = tflearn.ImageAugmentation()        # Random flip an image        img_aug.add_random_flip_leftright()        # Add these methods into an 'input_data' layer        network = input_data(shape=[None, 32, 32, 3],                             data_preprocessing=img_prep,                             data_augmentation=img_aug)