import tflearnfrom tflearn.layers.core import input_data, dropout, fully_connectedfrom tflearn.layers.conv import conv_2d, max_pool_2dfrom tflearn.datasets import cifar10# 权值微调， 在一个新任务上微调一个预训练的模型def finetuning():    # load data    (X, Y), (X_test, Y_test) = cifar10.load_data(dirname='./data', one_hot=True)    num_class = 10    # redefinition of convenet_cifar10 network    network = input_data(shape=[None, 32, 32, 3])    network = conv_2d(network, 32, 3, activation='relu')    network = max_pool_2d(network, 2)    network = dropout(network, 0.75)    network = conv_2d(network, 64, 3, activation='relu')    network = conv_2d(network, 64, 3, activation='relu')    network = max_pool_2d(network, 2)    network = dropout(network, 0.5)    network = fully_connected(network, 512, activation='relu')    network = dropout(network, 0.5)    # finetuning softmax layer    softmax = fully_connected(network, num_class, activation='softmax', restore=False)    regression = tflearn.layers.estimator.regression(softmax, optimizer='adam', loss='categorical_crossentropy',                            learning_rate=0.001)    model = tflearn.DNN(regression, checkpoint_path='model_finetuning',                        max_checkpoints=3, tensorboard_verbose=0)    # load pre-exiting model, restoring all weights, except softmax layer ones    # model.load('cifar10_cnn')    # start finetuning    model.fit(X, Y, n_epoch=1, validation_set=0.1, shuffle=True,              show_metric=True, batch_size=64, snapshot_step=200,              snapshot_epoch=False, run_id='model_finetuning')    model.save('model/cifar10_cnn')# 权重保持。保存和还原一个模型def weight_persistence():    import tflearn    import tflearn.datasets.mnist as mnist    X, Y, testX, testY = mnist.load_data(one_hot=True)    # model    input_layer = tflearn.input_data(shape=[None, 784], name='input')    dense1 = tflearn.fully_connected(input_layer, 128, name='dense1')    dense2 = tflearn.fully_connected(dense1, 256, name='dense2')    softmax = tflearn.fully_connected(dense2, 10, activation='softmax')    regression = tflearn.regression(softmax, optimizer='adam', learning_rate=0.001, loss='categorical_crossentropy')    # define classifer with model checkopoint    model = tflearn.DNN(regression, checkpoint_path='model.tfl.ckpt')    model.fit(X, Y, n_epoch=1, validation_set=(testX, testY),              show_metric=True,              snapshot_epoch=True,              snapshot_step=500,              run_id='model_and_weights')    '''        save and load a model    '''    model.save('model.tfl')    model.load('model.tfl')    # resume training    model.fit(X, Y, n_epoch=1,              validation_set=(testX, testY),              show_metric=True,              snapshot_epoch=True,              run_id='model_and_weights')    '''  retrieving weights '''    dense1_vars = tflearn.variables.get_layer_variables_by_name('dense1')    print("Dense1 layer weights")    print(model.get_weights(dense1_vars[0]))    print('dense1 layer biases')    with model.session.as_default():        print(tflearn.variables.get_value(dense1_vars[1]))    print("dense2 layer weights")    print(model.get_weights(dense2.W))    # Or using generic tflearn function:    print("Dense2 layer biases:")    with model.session.as_default():        print(tflearn.variables.get_value(dense2.b))if __name__ == '__main__':    finetuning()