import tensorflow as tfimport numpy as npimport shutilimport tensorflow as tf'''TensorFlow全新的数据读取方式：Dataset API入门教程    https://zhuanlan.zhihu.com/p/30751039https://www.tensorflow.org/api_docs/python/tf/data/DatasetDataset API是TensorFlow 1.3版本中引入的一个新的模块，主要服务于数据读取，构建输入数据的pipeline。此前，在TensorFlow中读取数据一般有两种方法：使用placeholder读内存中的数据使用queue读硬盘中的数据Dataset API同时支持从内存和硬盘的读取，相比之前的两种方法在语法上更加简洁易懂。'''def study_dataset():    dataset = tf.data.Dataset.from_tensor_slices(np.array([1.0, 2.0]))    # 1. apply:    # Apply a transformation function to this dataset    dataset = dataset.apply(lambda x: x)    # 2. batch:    #  batch(batch_size)    # 3. cache:    # cache(filename='')  Caches the elements in this dataset.    # 4. concatenate    # concatenate(dataset)    a = tf.data.Dataset.from_tensor_slices([1, 2, 3])    b = tf.data.Dataset.from_tensor_slices([4, 5, 6, 7])    a.concatenate(b) == {1, 2, 3, 4, 5, 6, 7}    # 5. filter    #  filter(predicate)    # 6. flat_map    # flat_map(map_func) Maps map_func across this dataset and flattens the result.    # 7. from_generator    passCSV_COLUMNS = ['fare_amount', 'pickuplon', 'pickuplat', 'dropofflon', 'dropofflat', 'passengers', 'key']LABEL_COLUMN = 'fare_amount'DEFAULTS = [[0.0], [-74.0], [40.0], [-74.0], [40.7], [1.0], ['nokey']]def read_dataset(filename, mode, batch_size=512):    def _input_fn():        def decode_csv(value_column):            columns = tf.decode_csv(value_column, record_defaults=DEFAULTS)            features = dict(zip(CSV_COLUMNS, columns))            label = features.pop(LABEL_COLUMN)            return features, label        # Create list of file names that match "glob" pattern (i.e. data_file_*.csv)        filenames_dataset = tf.data.Dataset.list_files(filename)        # Read lines from text files        textlines_dataset = filenames_dataset.flat_map(tf.data.TextLineDataset)        # Parse text lines as comma-separated values (CSV)        dataset = textlines_dataset.map(decode_csv)        # Note:        # use tf.data.Dataset.flat_map to apply one to many transformations (here: filename -> text lines)        # use tf.data.Dataset.map      to apply one to one  transformations (here: text line -> feature list)        if mode == tf.estimator.ModeKeys.TRAIN:            num_epochs = None  # indefinitely            dataset = dataset.shuffle(buffer_size=10 * batch_size)        else:            num_epochs = 1  # end-of-input after this        dataset = dataset.repeat(num_epochs).batch(batch_size)        return dataset.make_one_shot_iterator().get_next()    return _input_fndef get_train():    return read_dataset('../data/taxi-train.csv', mode=tf.estimator.ModeKeys.TRAIN)def get_valid():    return read_dataset('../data/taxi-valid.csv', mode=tf.estimator.ModeKeys.EVAL)def get_test():    return read_dataset('../data/taxi-test.csv', mode=tf.estimator.ModeKeys.EVAL)INPUT_COLUMNS = [    tf.feature_column.numeric_column('pickuplon'),    tf.feature_column.numeric_column('pickuplat'),    tf.feature_column.numeric_column('dropofflat'),    tf.feature_column.numeric_column('dropofflon'),    tf.feature_column.numeric_column('passengers'),]def add_more_features(feats):  # Nothing to add (yet!)  return featsfeature_cols = add_more_features(INPUT_COLUMNS)tf.logging.set_verbosity(tf.logging.INFO)OUTDIR = '../data/taxi_trained'shutil.rmtree(OUTDIR, ignore_errors = True) # start fresh each timemodel = tf.estimator.LinearRegressor(feature_columns = feature_cols, model_dir = OUTDIR)model.train(input_fn = get_train(), steps = 1000)def print_rmse(model, name, input_fn):  metrics = model.evaluate(input_fn = input_fn, steps = 1)  print('RMSE on {} dataset = {}'.format(name, np.sqrt(metrics['average_loss'])))print_rmse(model, 'validation', get_valid())if __name__ == '__main__':    study_dataset()    pass